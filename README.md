# FLAN-T5 Fine-Tuning for Generating Less Toxic Content

This project fine-tunes a FLAN-T5 model to generate less toxic content using Meta AI's hate speech reward model. The reward model is a binary classifier that predicts either "not hate" or "hate" for the given text. Proximal Policy Optimization (PPO) is utilized for fine-tuning the model and reducing its toxicity.

## Introduction

In this project, we aim to enhance the output generated by a FLAN-T5 model by reducing its toxicity. This is achieved through the following steps:

1. **Data Preparation**: We collect a dataset containing both toxic and non-toxic text samples for training the reward model.
2. **Training the Reward Model**: Meta AI's hate speech reward model is trained on the collected dataset to classify text into "not hate" or "hate" categories.
3. **Fine-Tuning FLAN-T5**: The FLAN-T5 model is fine-tuned using Proximal Policy Optimization (PPO) with the reward signal from the trained hate speech reward model.
4. **Evaluation**: The performance of the fine-tuned model is evaluated based on various metrics, including toxicity reduction and generation quality.

## Requirements

- Python >= 3.6
- PyTorch
- Transformers
- Meta AI's hate speech reward model
- FLAN-T5 model
- Dataset containing toxic and non-toxic text samples

## Usage

1. Clone the repository and navigate to the project directory.
2. Install the required dependencies listed in the `requirements.txt` file.
3. Prepare the dataset and preprocess the text samples.
4. Train the hate speech reward model using the prepared dataset.
5. Fine-tune the FLAN-T5 model using PPO and the trained reward model.
6. Evaluate the fine-tuned model's performance on the test set.
7. Adjust hyperparameters and experiment with different configurations to improve results if necessary.

## References

- Meta AI Hate Speech Reward Model: [Link](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target)
- FLAN-T5 Model: [Link](https://huggingface.co/docs/transformers/model_doc/flan-t5)
- Proximal Policy Optimization (PPO): [Documentation](https://spinningup.openai.com/en/latest/algorithms/ppo.html#proximal-policy-optimization)
- Dataset: [Link](https://huggingface.co/datasets/knkarthick/dialogsum)


Feel free to contribute to this project by forking and submitting pull requests!
